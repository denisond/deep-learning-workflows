# -*- coding: utf-8 -*-
"""train_optuna.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1srBIMKMBRukbPbNA6e0fDIbBXuBOQ0Vm
"""

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append('/content/drive/My Drive/Colab Notebooks/kaggle/MoA')
import utils_moa as utils

modulename = 'utils_moa'
if modulename not in sys.modules:
    print('You have not imported the {} module'.format(modulename))

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install optuna
# import optuna
# import torch
# import numpy as np
# import pandas as pd

# Config
ROOT = '/content/drive/My Drive/Colab Notebooks/kaggle/MoA/input'
DEVICE = "cuda"
EPOCHS = 4
BATCH_SIZE = 1024
KFOLDS = 5

def run_training(fold, params, save_model=False):
    df = pd.read_csv(f"{ROOT}/train_features.csv")
    df = utils.Engine.process_data(df) # no embedding layer cause 3 vars

    targets_df = pd.read_csv(f"{ROOT}/train_targets_folds.csv")

    # feature and target column names
    feature_columns = df.drop("sig_id", axis=1).columns
    target_columns = targets_df.drop(["sig_id", "kfold"], axis=1).columns

    df = df.merge(targets_df, on="sig_id", how="left")

    # create train and valid dfs
    train_df = df[df.kfold != fold].reset_index(drop=True)
    valid_df = df[df.kfold == fold].reset_index(drop=True)

    # split features and targets
    x_train = train_df[feature_columns].to_numpy()
    y_train = train_df[target_columns].to_numpy()

    x_valid = valid_df[feature_columns].to_numpy()
    y_valid = valid_df[target_columns].to_numpy()

    # create MoADataset instances
    train_dataset = utils.MoADataset(features=x_train, targets=y_train)
    valid_dataset = utils.MoADataset(features=x_valid, targets=y_valid)

    # create dataloaders from MoADatasets
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=BATCH_SIZE, 
        num_workers=8, shuffle=True # do we really want shuffle here
    )

    valid_loader = torch.utils.data.DataLoader(
        valid_dataset, batch_size=BATCH_SIZE, num_workers=8
    )

    model = utils.Model(
        nfeatures=x_train.shape[1], 
        ntargets=y_train.shape[1], 
        nlayers=params["num_layers"], 
        hidden_size=params["hidden_size"], 
        dropout=params["dropout"],
    )

    model.to(DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=params["learning_rate"])
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, threshold=0.00001, 
    #                                                        mode='min', verbose=True)
    eng = utils.Engine(model, optimizer, device=DEVICE)

    best_loss = np.inf
    early_stopping_iter = 10
    early_stopping_counter = 0

    for epoch in range(EPOCHS):
        train_loss = eng.train(train_loader)
        valid_loss = eng.evaluate(valid_loader)
        print(f"fold: {fold}, epoch: {epoch}, {train_loss}, {valid_loss}")
        if valid_loss < best_loss:
            best_loss = valid_loss
            if save_model:
                torch.save(model.state_dict(), f"{ROOT}/model_{fold}.bin")
        else:
            early_stopping_counter += 1


        if early_stopping_counter > early_stopping_iter:
            break
    
        scheduler.step() # check effectiveness of scheduler
    # models.append(model.state_dict()) # figure out how to best store models
    return best_loss

def objective(trial):
    # can add more params
    params = {
        "num_layers": trial.suggest_int("num_layers", 1, 7),
        "hidden_size": trial.suggest_int("hidden_size", 16, 2048),
        "dropout": trial.suggest_uniform("dropout", 0.1, 0.7),
        "learning_rate": trial.suggest_loguniform("learning_rate", 1e-6, 1e-3)
    }
    all_losses = []
    for f_ in range(KFOLDS):
        temp_loss = run_training(f_, params, save_model=False)
        all_losses.append(temp_loss)
    
    return np.mean(all_losses)

if __name__ == "__main__":
    study = optuna.create_study(direction="minimize")
    study.optimize(objective, n_trials=3)

    print("best trial:")
    trial_ = study.best_trial

    print(trial_.value)
    print(trial_.params)

    scores = 0
    for j in range(KFOLDS):
        scr = run_training(j, trial_.params, save_model=True)
        scores += scr
    
    print(f"final_cv: {scores / KFOLDS}")

# test_features = pd.read_csv('/content/drive/My Drive/Colab Notebooks/kaggle/MoA/input/test_features.csv')
# test_features = pd.concat([test_features, pd.get_dummies(test_features['cp_time'], prefix='cp_time')], axis=1)
# test_features = pd.concat([test_features, pd.get_dummies(test_features['cp_dose'], prefix='cp_dose')], axis=1)
# test_features = pd.concat([test_features, pd.get_dummies(test_features['cp_type'], prefix='cp_type')], axis=1)
# test_features = test_features.drop(['cp_type', 'cp_time', 'cp_dose'], axis=1)
# class TestMoADataset:
#     def __init__(self, dataset):
#         self.dataset = dataset
    
#     def __len__(self):
#         return self.dataset.shape[0]
    
#     def __getitem__(self, item):
#         return {
#             "x": torch.tensor(self.dataset[item, :], dtype=torch.float)
#         }
# test_dataset = TestMoADataset(dataset=test_features.iloc[:, 1:].values)
# # num_workers=0?
# test_loader = torch.utils.data.DataLoader(
#             test_dataset,
#             batch_size=1024,
#             num_workers=0,
#             shuffle=False
#         )
# predictions = np.zeros((test_features.shape[0], 206))
# inference_model = model.model
# inference_model.eval()

# for ind, batch in enumerate(test_loader):
#     p = inference_model(batch['x'])[0].detach().cpu().numpy()
#     predictions[ind * 1024:(ind + 1) * 1024] * p
# test_features1 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/kaggle/MoA/input/test_features.csv')
# s = pd.DataFrame({'sig_id': test_features1['sig_id'].values})
# for col in train_targets_scored.columns[1:].tolist():
#     s[col] = 0
# s.loc[:, train_targets_scored.columns[1:]] = predictions
# s.loc[s['sig_id'].isin(test_features1.loc[test_features1['cp_type'] == 'ctl_vehicle', 'sig_id']), train_targets_scored.columns[1:]] = 0
# s.to_csv('submission.csv', index=False)
# torch.save(model.model.state_dict(),'model.pt')

